{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "",
  "signature": "sha256:2feabed8d5a274b5a79dfd6ddac195ae76116d05ce6348361fd3fd1cc0153382"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -\"- coding: utf-8 -\"-\n",
      "from __future__ import division\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "#import scipy\n",
      "from gensim import corpora, models, similarities\n",
      "\n",
      "raw = pd.read_csv('C:/Users/Tariq/Dropbox (MIT)/Projects/3. ITS/Sentiment Analysis/Eshrag Corpus/GS-positiveVSnegativeVSneutral-6514twt_Anoo3Rev_33feat.csv', sep=',',header=0)\n",
      "clean = pd.read_csv('C:/Users/Tariq/Dropbox (MIT)/Projects/3. ITS/Sentiment Analysis/Eshrag Corpus/V2-GS-R&R-TwitterSentimentCorpus2014-Ar.csv', sep=',',header=0)\n",
      "\n",
      "text = raw['tweetTXT_Ar_surface'].values\n",
      "query_term = raw['query-term'].values\n",
      "tweet_category = clean['tweet-category'].values\n",
      "# Get arabic words only, i.e. remove username-hashtag.  Change query term to the actual word in query term list."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate a seperate document for each topic\n",
      "topics = np.unique(tweet_category)\n",
      "docs = {}\n",
      "for t in topics:\n",
      "    docs[t] = \"\"\n",
      "    \n",
      "# Assign tweets from each topic to its corresponding document\n",
      "for i in range(0,len(text)):\n",
      "    docs[tweet_category[i]] = docs[tweet_category[i]] + text[i] + query_term[i] + \" \"\n",
      "\n",
      "# Generate a list of lists of words in all documents + Filter out undesired words\n",
      "undesiredWords = set(\"for a of the and to in 'user-name url ' hashtag 'hashtag query-term\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "texts = [[word for word in docs[key].lower().split() if word not in undesiredWords] for key in docs.keys()]\n",
      "# remove words that appear only once\n",
      "from collections import defaultdict\n",
      "frequency = defaultdict(int)\n",
      "for text in texts:\n",
      "    for token in text:\n",
      "        frequency[token] += 1\n",
      "\n",
      "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
      "\n",
      "#from pprint import pprint   # pretty-printer\n",
      "#pprint(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# More Words Filtering, detailed explanation at the function def at the end of this docment\n",
      "\n",
      "texts = [[Word_filtering(token) for token in text if Word_filtering(token) ] for text in texts]\n",
      "\n",
      "#texts = Remove_popular_words(texts, 0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# printing lengths of list for checking\n",
      "for d in docs.keys():\n",
      "    print(d)\n",
      "print(docs['locations'][0:20])\n",
      "print(\"-----------\")\n",
      "for t in texts:\n",
      "    print(len(t))\n",
      "print(\"-----------\")\n",
      "#for t in texts:\n",
      "#    print(len(set(t)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "dictionary = corpora.Dictionary(texts)\n",
      "dictionary.save('C:/Users/Tariq/Dropbox (MIT)/Projects/3. ITS/Visualization_Summer_2016/arabic_tweets.dict')\n",
      "\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "corpora.MmCorpus.serialize('C:/Users/Tariq/Dropbox (MIT)/Projects/3. ITS/Visualization_Summer_2016/arabic_tweets.mm', corpus) \n",
      "# store to disk, for later use"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=9)\n",
      "lda.print_topics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = similarities.MatrixSimilarity(lda[corpus])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Test Dataset\n",
      "testData = pd.read_csv('C:/Users/Tariq/Dropbox (MIT)/Projects/3. ITS/Visualization_Summer_2016/tweets_with_time3.csv', sep=',',header=0)\n",
      "tweets = testData['content'].values\n",
      "#tweets = fix_unicode(tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#store the topic ID \n",
      "\n",
      "topics = []\n",
      "for tweet in tweets:\n",
      "    vec_bow = dictionary.doc2bow(tweet.lower().split())\n",
      "    s = lda.get_document_topics(vec_bow)\n",
      "    s = sorted(s, key=lambda item: -item[1])\n",
      "    topics.append(s[0][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Word_filtering(word):\n",
      "    \"\"\"returns word if the word meets the following conditions, False otherwise.\n",
      "        1. The word is in Arabic and has no encoding issues.\n",
      "        2. word length is more than two characters\n",
      "    \"\"\"\n",
      "    l_words = re.findall(r'[\\u0600-\\u06FF]+', word)\n",
      "    if len(l_words) > 0:\n",
      "        new_word = l_words[0]\n",
      "        if len(new_word) > 2:\n",
      "            return new_word\n",
      "    return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def Remove_popular_words(texts, threshold = 0.5):\n",
      "    \"\"\" This functions receives a data list of text documents and returns the list after removing words \n",
      "        that occur in more than n % of the documents. 'n' is controled by setting the threshold parameter.\n",
      "        'threshold' is expected to be in decimal format\n",
      "    \"\"\"\n",
      "    unique_texts = texts                   # Getting unique words in each document\n",
      "    for t in range(0,len(texts)):\n",
      "        unique_texts[t] = set(texts[t])\n",
      "    \n",
      "    freq = defaultdict(int)\n",
      "    for text in unique_texts:\n",
      "        for token in text:\n",
      "            freq[token] += 1\n",
      "            \n",
      "    total = len(texts)\n",
      "    texts = [[token for token in text if (freq[token]/total) <= threshold] for text in texts]\n",
      "    \n",
      "    return texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fix_unicode(text):\n",
      "    words = []\n",
      "    for t in range(0,len(text)):\n",
      "        text[t] = re.sub(\"<U\\+([0-9A-F]{4})>\", r\"\\u\\\\1\", text[t])\n",
      "        #print t\n",
      "        #text[t] = text[t].decode('unicode_escape')\n",
      "        words.append(text[t].split())\n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "unique_texts = texts                   # Getting unique words in each document\n",
      "for t in range(0,len(texts)):\n",
      "    unique_texts[t] = set(texts[t])\n",
      "    \n",
      "    \n",
      "freq = defaultdict(int)\n",
      "for text in unique_texts:\n",
      "    for token in text:\n",
      "        freq[token] += 1\n",
      "        count = count + 1\n",
      "\n",
      "print(len(freq.keys()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "texts2 = [[token for token in text if (freq[token]/9) <= 0.5] for text in texts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "for i in range(0,len(texts)):\n",
      "    for w in texts[i]:\n",
      "        if w not in texts2[i]:\n",
      "            print(w)\n",
      "            count = count + 1\n",
      "\n",
      "print(count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}