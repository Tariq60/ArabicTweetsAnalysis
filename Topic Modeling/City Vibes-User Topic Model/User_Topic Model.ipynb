{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -\"- coding: utf-8 -\"-\n",
    "from __future__ import division\n",
    "import re\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Tariq\\\\Dropbox (MIT)\\\\Projects\\\\3. ITS\\\\User Topic Model'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "#import gensim\n",
    "#from snowballstemmer import stemmer\n",
    "#ar_stemmer = stemmer(\"arabic\")\n",
    "#ar_stemmer.stemWord(u\"فسميتموها\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = pickle.load(open(\"Userdocs.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = open(\"C:\\\\Users\\\\Tariq\\\\Dropbox (MIT)\\\\Projects\\\\3. ITS\\\\Arabic NLP Resources\\\\stopwords.txt\",\"rb\").readlines()\n",
    "for i in range(len(stopwords)):\n",
    "    stopwords[i] = stopwords[i].decode('utf8')\n",
    "    stopwords[i] = stopwords[i].split(\"\\r\")[0]\n",
    "stopwords = stopwords[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[token for token in text if \"#\" not in token] for text in texts]\n",
    "texts = [[token for token in text if \"_\" not in token] for text in texts]\n",
    "texts = [[re.sub(\"[^ء-غف-ي]\", \"\", token) for token in text] for text in texts]\n",
    "texts = [[token for token in text if len(token) > 2] for text in texts]\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "texts = [[token for token in text if token not in stopwords] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#texts = [[token for token in text if token not in too_common] for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9913"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=10)\n",
    "corpus_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.0178509007\n"
     ]
    }
   ],
   "source": [
    "#print(lda.top_topics(corpus))\n",
    "print(lda.log_perplexity(corpus))\n",
    "\n",
    "words = lda.show_topics()\n",
    "topics = []\n",
    "for word in words:\n",
    "    topics.append(re.sub(\"[^ء-غف-ي]\", \" \", word[1]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['اللهم', 'اليوم', 'الناس', 'شكرا', 'الله', 'محمد']"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 10\n",
    "n_words = 10\n",
    "counts = [[0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "for i in range(n_topics):\n",
    "    for j in range(n_words):\n",
    "        word = topics[i][j]\n",
    "        for topic in topics:\n",
    "            if word in topic:\n",
    "                counts[i][j] += 1\n",
    "\n",
    "too_common = []\n",
    "for i in range(n_topics):\n",
    "    for j in range(n_words):\n",
    "        if counts[i][j] > 5:\n",
    "            too_common.append(topics[i][j])\n",
    "\n",
    "too_common = list(set(too_common))\n",
    "too_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['الله', 'اللهم', 'شكرا', 'اليوم', 'الناس', 'السعودية', 'محمد', 'لله', 'الف', 'بخير']\n",
      "['الله', 'صورة', 'مسابقة', 'خاص', 'باسمك', 'هذي', 'بطولة', 'تفضل', 'شكرا', 'لقميص']\n",
      "['الله', 'اللهم', 'شكرا', 'رحمه', 'اليوم', 'انا', 'محمد', 'الامام', 'الناس', 'الف']\n",
      "['الله', 'شكرا', 'اللهم', 'الناس', 'اليوم', 'انت', 'الرابط', 'خير', 'محمد', 'اللي']\n",
      "['الله', 'اللهم', 'اليوم', 'انا', 'الناس', 'لله', 'خير', 'محمد', 'انت', 'والله']\n",
      "['الله', 'اللهم', 'اليوم', 'محمد', 'شكرا', 'الناس', 'السعودية', 'انا', 'الاول', 'خير']\n",
      "['الله', 'اللهم', 'لقميص', 'تفضل', 'صورة', 'بطولة', 'اليوم', 'خاص', 'محمد', 'الناس']\n",
      "['الله', 'اليوم', 'اللهم', 'محمد', 'شكرا', 'الاول', 'القدم', 'عبر', 'الناس', 'السعودية']\n",
      "['الله', 'اللهم', 'شكرا', 'اليوم', 'انا', 'محمد', 'عبر', 'اللي', 'صورة', 'خير']\n",
      "['الله', 'اللهم', 'محمد', 'رحمه', 'الناس', 'ليس', 'اليوم', 'عام', 'السعودية', 'خير']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[10, 9, 7, 9, 8, 4, 9, 2, 2, 1],\n",
       " [10, 3, 1, 2, 1, 1, 2, 2, 7, 2],\n",
       " [10, 9, 7, 2, 9, 4, 9, 1, 8, 2],\n",
       " [10, 7, 9, 8, 9, 2, 1, 5, 9, 2],\n",
       " [10, 9, 9, 4, 8, 2, 5, 9, 2, 1],\n",
       " [10, 9, 9, 9, 7, 8, 4, 4, 2, 5],\n",
       " [10, 9, 2, 2, 3, 2, 9, 2, 9, 8],\n",
       " [10, 9, 9, 9, 7, 2, 1, 2, 8, 4],\n",
       " [10, 9, 7, 9, 4, 9, 2, 2, 3, 5],\n",
       " [10, 9, 9, 2, 8, 1, 9, 1, 4, 5]]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(topic)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['اللهم', 'اليوم', 'انا', 'شكرا', 'الله', 'محمد']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topic distributions per documents with unused topics set to zero\n",
    "doc_topics = [[]] * 111\n",
    "j = 0\n",
    "for doc in corpus_lda:\n",
    "    doc_topics[j] = [0] * lda.num_topics\n",
    "    i = 0\n",
    "    while(i<len(doc)):\n",
    "        temp = doc[i]\n",
    "        doc_topics[j][temp[0]]=format(temp[1],'.4f')\n",
    "        i = i + 1\n",
    "    j = j + 1\n",
    "\n",
    "#print(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"infleuntial_users_Saudi.csv\", header=0)\n",
    "labels = labels['Type']\n",
    "doc_data = pd.DataFrame(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_in_directory = 111\n",
    "tweets = []\n",
    "types = []\n",
    "for i in range(files_in_directory):\n",
    "    user = pd.read_csv(\"User\"+str(i)+\".txt\", sep=';', header=0, index_col = False)\n",
    "    usertweets = user['text'].values\n",
    "    usertype = user['type'].values\n",
    "    if len(tweets) > 1000 : \n",
    "        usertweets = usertweets[0:1000]\n",
    "        usertype = usertype[0:1000]\n",
    "    for i in range(len(usertweets)):\n",
    "        tweets.append(usertweets[i])\n",
    "        types.append(usertype[i])\n",
    "\n",
    "# preprocess incoming Labeled user tweets before classifying\n",
    "for i in range(0,len(tweets)):\n",
    "    if type(tweets[i]) == str:\n",
    "        tweets[i] = re.findall(\"[ء-غف-ي]+\",tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets2 = []\n",
    "types2 = []\n",
    "for i in range(len(tweets)):\n",
    "    if type(tweets[i]) != float:\n",
    "        if len(tweets[i]) > 4:\n",
    "            tweets2.append(tweets[i])\n",
    "            types2.append(types[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract Features from Geotagged Tweets    (i.e. apply LDA model)\n",
    "tweet_corpus = [dictionary.doc2bow(text) for text in tweets2]\n",
    "tweet_lda = lda[tweet_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topic distributions per documents with unused topics set to zero\n",
    "tweet_topics = [[]] * len(tweet_lda)\n",
    "j = 0\n",
    "for doc in tweet_lda:\n",
    "    tweet_topics[j] = [0] * lda.num_topics\n",
    "    i = 0\n",
    "    while(i<len(doc)):\n",
    "        temp = doc[i]\n",
    "        tweet_topics[j][temp[0]]=format(temp[1],'.4f')\n",
    "        i = i + 1\n",
    "    j = j + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.Series(types2)\n",
    "tweet_data = pd.DataFrame(tweet_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit a logistic regression model and store the class predictions\n",
    "\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "X = doc_data\n",
    "y = labels\n",
    "logreg.fit(X, y)\n",
    "\n",
    "predictions = logreg.predict(X)\n",
    "\n",
    "#data['labels']= labels.values\n",
    "#data['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36936936936936937\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(0,len(predictions)):\n",
    "    if labels[i] == predictions[i]:\n",
    "        c += 1\n",
    "print(c/111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = [0] * 20\n",
    "mul = 5\n",
    "for n in range(0,20): \n",
    "    lda = models.LdaModel(corpus, id2word=dictionary, num_topics=(n+1)*mul)\n",
    "    lda.print_topics()\n",
    "    corpus_lda = lda[corpus]\n",
    "    \n",
    "    doc_topics = [[]] * 97\n",
    "    j = 0\n",
    "    for doc in corpus_lda:\n",
    "        doc_topics[j] = [0] * lda.num_topics\n",
    "        i = 0\n",
    "        while(i<len(doc)):\n",
    "            temp = doc[i]\n",
    "            doc_topics[j][temp[0]]=format(temp[1],'.4f')\n",
    "            i = i + 1\n",
    "        j = j + 1\n",
    "\n",
    "    data = pd.DataFrame(doc_topics)\n",
    "\n",
    "    logreg = LogisticRegression(C=1e9)\n",
    "    X = data\n",
    "    y = labels\n",
    "    logreg.fit(X, y)\n",
    "    predictions = logreg.predict(X)\n",
    "    \n",
    "    accuracy[n] = 0\n",
    "    for m in range(0,97):\n",
    "        #print(labels[m],\"\\t\\t\",predictions[m])\n",
    "        if predictions[m] == labels[m]:\n",
    "            accuracy[n] = accuracy[n] + 1\n",
    "    accuracy[n] = accuracy[n] / 97\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "x = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "y = accuracy\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "#ax1.scatter(data[0], , s=10, c='b', marker=\"s\", label='first')\n",
    "ax1.scatter(x,y, s=10, c='r', marker=\"o\")\n",
    "#plt.legend(loc='upper left');\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = open('C:/Users/Tariq/Dropbox (MIT)/Projects/3. ITS/Hashtags Topic Model/AllTweets.csv',\n",
    "               encoding='utf-8', errors=\"ignore\")\n",
    "tweets = sample.read()\n",
    "tweets = tweets.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125456 501829\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "i = 5\n",
    "content = []\n",
    "while i < len(tweets):\n",
    "    content.append(tweets[i])\n",
    "    i += 4\n",
    "    c += 1\n",
    "\n",
    "print(c, len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess incoming Geotagged tweets before classifying\n",
    "for i in range(0,len(content)):\n",
    "    content[i] = content[i].replace(\"ـ\",\"\")\n",
    "    content[i] = re.findall(\"[ء-غف-ي]+\",content[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keeping words of length 3 and above!\n",
    "content = [[token for token in text if len(token)>= 3] for text in content]\n",
    "# keeping tweets with more than 5 words\n",
    "content = [text for text in content if len(text) > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract Features from Geotagged Tweets    (i.e. apply LDA model)\n",
    "geo_corpus = [dictionary.doc2bow(text) for text in content]\n",
    "geo_lda = lda[geo_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50669"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topic distributions per documents with unused topics set to zero\n",
    "geo_topics = [[]] * len(geo_lda)\n",
    "j = 0\n",
    "for doc in geo_lda:\n",
    "    geo_topics[j] = [0] * lda.num_topics\n",
    "    i = 0\n",
    "    while(i<len(doc)):\n",
    "        temp = doc[i]\n",
    "        geo_topics[j][temp[0]]=format(temp[1],'.4f')\n",
    "        i = i + 1\n",
    "    j = j + 1\n",
    "\n",
    "geo_data = pd.DataFrame(geo_topics)\n",
    "geo_predictions = logreg.predict(geo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "content2, lat, lon = [], [], []\n",
    "while i < len(tweets):\n",
    "    content2.append(tweets[i])\n",
    "    lat.append(tweets[i+1])\n",
    "    lon.append(tweets[i+2])\n",
    "    i += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess incoming Geotagged tweets before classifying\n",
    "for i in range(0,len(content2)):\n",
    "    content2[i] = content2[i].replace(\"ـ\",\"\")\n",
    "    content2[i] = re.findall(\"[ء-غف-ي]+\",content2[i])\n",
    "\n",
    "content2 = [[token for token in text if len(token)>= 3] for text in content2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_data = []\n",
    "exp_data.append([])\n",
    "exp_data.append([])\n",
    "exp_data.append([])\n",
    "j = 0\n",
    "for i in range(0,len(content2)):\n",
    "    if len(content2[i]) > 4:\n",
    "        exp_data[0].append(lat[i])\n",
    "        exp_data[1].append(lon[i])\n",
    "        exp_data[2].append(geo_predictions[j])\n",
    "        j += 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_data = []\n",
    "j = 0\n",
    "for i in range(0,len(content2)):\n",
    "    if len(content2[i]) > 4:\n",
    "        exp_data.append((content2[i], lat[i], lon[i], geo_predictions[j]))\n",
    "        j += 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_csv = pd.DataFrame(exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_csv.to_csv(\"geoTaggedPredictions2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('C:\\\\Users\\\\Tariq\\\\Dropbox (MIT)\\\\Projects\\\\3. ITS\\\\zeyad-geo_tagged.txt','rb') \n",
    "gTweets = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-264-dbb788c7d5fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgTweets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mgText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"text\": \"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\", \"place\":'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mlat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"coordinates\": [ '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"coordinates\": [ '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' ]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "gText = []\n",
    "lat = []\n",
    "lon = []\n",
    "for tweet in gTweets:\n",
    "    gText.append(tweet.decode('utf-8').split('\"text\": \"')[1].split('\", \"place\":')[0])\n",
    "    lat.append(str(tweet).split('\"coordinates\": [ ')[1].split(',')[0])\n",
    "    lon.append(str(tweet).split('\"coordinates\": [ ')[1].split(',')[1].split(' ]')[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing if old code works on new dataset\n",
    "\n",
    "# preprocess incoming Geotagged tweets before classifying\n",
    "for i in range(0,len(gText)):\n",
    "    gText[i] = gText[i].replace(\"ـ\",\"\")\n",
    "    gText[i] = re.findall(\"[ء-غف-ي]+\",gText[i])\n",
    "    \n",
    "# keeping words of length 3 and above!\n",
    "gText = [[token for token in text if len(token)>= 3] for text in gText]\n",
    "# keeping tweets with more than 5 words\n",
    "gText = [text for text in gText if len(text) > 4]\n",
    "\n",
    "# Extract Features from Geotagged Tweets    (i.e. apply LDA model)\n",
    "geo_corpus2 = [dictionary.doc2bow(text) for text in gText]\n",
    "geo_lda2 = lda[geo_corpus2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['فلورنزي', 'آخر', 'ضحايا', 'الرباط', 'الصليبي', 'الأمامي']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gText[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topic distributions per documents with unused topics set to zero\n",
    "geo_topics2 = [[]] * len(geo_lda2)\n",
    "j = 0\n",
    "for doc in geo_lda2:\n",
    "    geo_topics2[j] = [0] * lda.num_topics\n",
    "    i = 0\n",
    "    while(i<len(doc)):\n",
    "        temp = doc[i]\n",
    "        geo_topics2[j][temp[0]]=format(temp[1],'.4f')\n",
    "        i = i + 1\n",
    "    j = j + 1\n",
    "\n",
    "geo_data2 = pd.DataFrame(geo_topics2)\n",
    "geo_predictions2 = logreg.predict(geo_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-268-02e0a2457711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgText2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgTweets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mgText2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"text\": \"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\", \"place\":'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# preprocess incoming Geotagged tweets before classifying\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "gText2 = []\n",
    "for tweet in gTweets:\n",
    "    gText2.append(tweet.decode('utf-8').split('\"text\": \"')[1].split('\", \"place\":')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess incoming Geotagged tweets before classifying\n",
    "for i in range(0,len(gText2)):\n",
    "    gText2[i] = gText2[i].replace(\"ـ\",\"\")\n",
    "    gText2[i] = re.findall(\"[ء-غف-ي]+\",gText2[i])\n",
    "    \n",
    "# keeping words of length 3 and above!\n",
    "gText2 = [[token for token in text if len(token)>= 3] for text in gText2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_data2 = []\n",
    "j = 0\n",
    "for i in range(0,len(gText2)):\n",
    "    if len(gText2[i]) > 4:\n",
    "        exp_data2.append((gText2[i], lat[i], lon[i], geo_predictions2[j]))\n",
    "        j += 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_csv2 = pd.DataFrame(exp_data2)\n",
    "exp_csv2.to_csv(\"geoTaggedPredictions_new.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
